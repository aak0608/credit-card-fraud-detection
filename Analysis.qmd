---
title: "Credit card fraud detection"
format: html
editor: visual
---


# Introduction

Credit card fraud is a growing concern in the financial industry, causing significant losses for banks and customers alike. With the rise of online transactions and digital payments, fraudsters have become increasingly sophisticated in exploiting security vulnerabilities. Detecting fraudulent transactions is a challenging task due to the highly imbalanced nature of fraud datasets, where fraudulent transactions represent only a small fraction of total transactions.

The dataset used in this analysis is derived from real-world credit card transactions and contains anonymized features extracted using Principal Component Analysis (PCA). The goal is to build predictive models that can accurately distinguish between fraudulent and legitimate transactions, helping financial institutions prevent financial losses and enhance security measures.


```{r echo=FALSE, warning=FALSE, message=FALSE}
# Load necessary libraries
library(tidyverse)
library(data.table)
library(ggplot2)
library(caret)
library(ROSE)
library(PRROC)
library(xgboost)
library(gridExtra)
library(patchwork)
library(DT)
library(kableExtra)
library(randomForest)
```

# Reading the data

```{r echo=FALSE, warning=FALSE, message=FALSE}
## Load Dataset
data <- fread("data/creditcard.csv")
```

```{r echo=FALSE, warning=FALSE, message=FALSE}
data %>% head() %>% DT::datatable()
```

# About the data

## Columns in the Credit Card Fraud Detection Dataset

| Column Name | Description |
|-------------|-------------|
| Time        | Seconds elapsed between this transaction and the first transaction in the dataset. |
| V1 to V28   | Anonymized features resulting from a PCA transformation. |
| Amount      | The transaction amount. |
| Class       | Target variable (0 for legitimate transactions, 1 for fraudulent transactions). |

:::{.callout-note}

The data has the dimensions of `r dim(data)`.

:::

## Exploring the data

```{r echo=FALSE, warning=FALSE, message=FALSE}
summary(data) %>% DT::datatable() # Summary statistics
```


### Check Class Imbalance

```{r echo=FALSE, warning=FALSE, message=FALSE}


new_table <- table(data$Class)

kable(new_table, format = "html", table.attr = "class='table'") %>% 
  kable_styling(full_width = FALSE, bootstrap_options = c("striped", "hover"), position = "center") %>%
  column_spec(1, border_right = TRUE) %>%  # Add a line between columns
  row_spec(0, bold = TRUE, color = "white", background = "lightblue")

```

### Visualizing Class Distribution

```{r echo=FALSE, warning=FALSE, message=FALSE}

ggplot(data, aes(x = factor(Class), fill = factor(Class))) +
  geom_bar(color = "black", size = 0.5) +
  scale_fill_manual(values = c("#0073C2FF", "#EFC000FF")) +
  theme_minimal() +
  labs(title = "Class Distribution of Transactions", x = "Class (0: Non-Fraud, 1: Fraud)", y = "Count") +
  theme(plot.title = element_text(hjust = 0.5, face = "bold", size = 14),
        axis.title = element_text(face = "bold"),
        legend.position = "none") +
  scale_y_continuous(labels = scales::comma)
```

### Correlation Heatmap

```{r echo=FALSE, warning=FALSE, message=FALSE}

ggplot(data, aes(x = V1, y = V2, color = as.factor(Class))) +
  geom_point(alpha = 0.5) +
  theme_minimal() +
  labs(title = "PCA Components V1 vs V2", color = "Class")
```

### Density Plot for Amount

```{r echo=FALSE, warning=FALSE, message=FALSE}

ggplot(data, aes(x = Amount, fill = as.factor(Class))) +
  geom_density(alpha = 0.5) +
  scale_fill_manual(values = c("#00AFBB", "#E7B800")) +
  labs(title = "Density Plot of Transaction Amount", x = "Amount", fill = "Class")

```


### Boxplot to detect anomalies

```{r echo=FALSE, warning=FALSE, message=FALSE}

ggplot(data, aes(x = factor(Class), y = Amount, fill = factor(Class))) +
  geom_boxplot(outlier.colour = "red", outlier.shape = 16) +
  labs(title = "Transaction Amounts by Class", x = "Class", y = "Amount") +
  theme_minimal()
```

## Data Preprocessing

```{r echo=FALSE, warning=FALSE, message=FALSE}
# Normalize Amount
scaler <- preProcess(data[, .(Amount)], method = "range")
data$Amount <- predict(scaler, data[, .(Amount)])

# Remove Time column
data <- data[, -c("Time")]

data %>% head() %>% DT::datatable()
```

We first normalise the amount then remove `Time` column.

```{r echo=FALSE, warning=FALSE, message=FALSE}
## Splitting Data
set.seed(123)
index <- createDataPartition(data$Class, p = 0.8, list = FALSE)
train_data <- data[index, ]
test_data <- data[-index, ]

## Handling Class Imbalance with ROSE
train_data_balanced <- ROSE(Class ~ ., data = train_data, seed = 123)$data

bal_table <- table(train_data_balanced$Class)  # Checking new balance

kable(bal_table, format = "html", table.attr = "class='table'") %>% 
  kable_styling(full_width = FALSE, bootstrap_options = c("striped", "hover"), position = "center") %>%
  column_spec(1, border_right = TRUE) %>%  # Add a line between columns
  row_spec(0, bold = TRUE, color = "white", background = "lightblue")
```

# Model Training

```{r echo=FALSE, warning=FALSE, message=FALSE}
str(train_data_balanced$Class)  # Should show "Factor w/ 2 levels"

```


## Logistic Regression
```{r echo=FALSE, warning=FALSE, message=FALSE}
## Model Training & Comparison
logistic_model <- glm(Class ~ ., data = train_data_balanced, family = binomial)
logistic_preds <- predict(logistic_model, test_data, type = "response")
logistic_class <- ifelse(logistic_preds > 0.5, 1, 0)
logistic_cm <- confusionMatrix(factor(logistic_class), factor(test_data$Class))
logistic_pr <- pr.curve(scores.class0 = logistic_preds, weights.class0 = test_data$Class, curve = TRUE)

print(logistic_cm)
```

### Logistic Regression Precision-Recall Curve
```{r echo=FALSE, warning=FALSE, message=FALSE}
# PR Curves
plot(logistic_pr, main = "Logistic Regression Precision-Recall Curve")
```

## Random Forest

```{r echo=FALSE, warning=FALSE, message=FALSE}
train_data_balanced$Class <- as.factor(train_data_balanced$Class)
rf_model <- randomForest(Class ~ ., data = train_data_balanced, ntree = 100)
rf_preds <- predict(rf_model, test_data, type = "prob")[,2]
rf_class <- ifelse(rf_preds > 0.5, 1, 0)
rf_cm <- confusionMatrix(factor(rf_class), factor(test_data$Class))
rf_pr <- pr.curve(scores.class0 = rf_preds, weights.class0 = test_data$Class, curve = TRUE)
rf_importance <- varImp(rf_model)

print(rf_cm)
```

```{r echo=FALSE, warning=FALSE, message=FALSE}
plot(rf_pr, main = "Random Forest Precision-Recall Curve")
```


## XGBoost

```{r echo=FALSE, warning=FALSE, message=FALSE}
## Splitting Data
set.seed(123)
index <- createDataPartition(data$Class, p = 0.8, list = FALSE)
train_data <- data[index, ]
test_data <- data[-index, ]

## Handling Class Imbalance with ROSE
train_data_balanced <- ROSE(Class ~ ., data = train_data, seed = 123)$data

table(train_data_balanced$Class)  # Checking new balance
```

### Model Training and Evaluation

```{r echo=FALSE, warning=FALSE, message=FALSE}
## Model Training - XGBoost
set.seed(123)
train_matrix <- xgb.DMatrix(data = as.matrix(train_data_balanced[, !names(train_data_balanced) %in% "Class"]), 
                            label = train_data_balanced$Class)

test_matrix <- xgb.DMatrix(data = as.matrix(test_data[, -"Class", with = FALSE]), 
                           label = test_data$Class)

params <- list(
  objective = "binary:logistic",
  eval_metric = "aucpr",
  max_depth = 6,
  eta = 0.1
)

model <- xgb.train(params = params, data = train_matrix, nrounds = 100)
```

```{r echo=FALSE, warning=FALSE, message=FALSE}
## Predictions
preds <- predict(model, test_matrix)
preds_class <- ifelse(preds > 0.5, 1, 0)
```


```{r echo=FALSE, warning=FALSE, message=FALSE}
## Evaluation
conf_matrix <- confusionMatrix(factor(preds_class), factor(test_data$Class))
auprc <- pr.curve(scores.class0 = preds, weights.class0 = test_data$Class, curve = TRUE)

print(conf_matrix)
```

### Area Under the Precision-Recall Curve

```{r echo=FALSE, warning=FALSE, message=FALSE}
plot(auprc)
```


# Discussion of Results

- Logistic Regression performs moderately well but struggles with imbalanced data.
- Random Forest improves performance by capturing complex interactions.
- XGBoost delivers the highest precision-recall score, making it the most effective for fraud detection.


